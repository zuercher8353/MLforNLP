{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Backpropagation in neural_networks Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYnJWqSo7-e1"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsC-zkNp7-e7"
      },
      "source": [
        "\n",
        "Neural Networks\n",
        "===============\n",
        "\n",
        "Neural networks can be constructed using the ``torch.nn`` package.\n",
        "\n",
        "Now that you had a glimpse of ``autograd``, ``nn`` depends on\n",
        "``autograd`` to define models and differentiate them.\n",
        "An ``nn.Module`` contains layers, and a method ``forward(input)``\\ that\n",
        "returns the ``output``.\n",
        "\n",
        "------------------\n",
        "\n",
        "Letâ€™s define this network:\n",
        "This network has 2 linear layer with sigmoid activation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izoco3g27-e8",
        "outputId": "0662b7bc-caec-4885-f57a-6acc0e1a0f46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 2) \n",
        "        self.fc2 = nn.Linear(2, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.sigmoid(self.fc1(x))\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=2, out_features=2, bias=True)\n",
            "  (fc2): Linear(in_features=2, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRNCynz7-feJ"
      },
      "source": [
        "Printing the weights of the Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQS-0OBK7-e_",
        "outputId": "dd28c570-9581-4a54-f304-d13ac965dfe8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "params = list(net.parameters())\n",
        "print(params)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parameter containing:\n",
            "tensor([[ 0.3649, -0.3526],\n",
            "        [-0.5054,  0.0098]], requires_grad=True), Parameter containing:\n",
            "tensor([ 0.0160, -0.6365], requires_grad=True), Parameter containing:\n",
            "tensor([[-0.2593, -0.1377],\n",
            "        [ 0.2917,  0.1411]], requires_grad=True), Parameter containing:\n",
            "tensor([0.2468, 0.5889], requires_grad=True)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Usxiepq3-nRh"
      },
      "source": [
        "Setting the weights to some initial value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kS7hfBL_txV",
        "outputId": "4f6f0d1a-42d0-423f-ce1f-9aad3711a78b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "with torch.no_grad():\n",
        "    \n",
        "    net.fc1.weight=torch.nn.Parameter(torch.tensor([[0.15,0.2],[0.25,0.3]]))\n",
        "    net.fc1.bias=torch.nn.Parameter(torch.tensor([0.35,0.35]))\n",
        "    print(net.fc1.weight)\n",
        "    print(net.fc1.bias)\n",
        "    net.fc2.weight=torch.nn.Parameter(torch.tensor([[0.4,0.45],[0.5,0.55]]))\n",
        "    net.fc2.bias=torch.nn.Parameter(torch.tensor([0.6,0.6]))\n",
        "    print(net.fc2.weight)\n",
        "    print(net.fc2.bias)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[0.1500, 0.2000],\n",
            "        [0.2500, 0.3000]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.3500, 0.3500], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[0.4000, 0.4500],\n",
            "        [0.5000, 0.5500]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.6000, 0.6000], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqcSKEdW-s3y"
      },
      "source": [
        "Network Forward Pass "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2x54x4gn7-fI",
        "outputId": "00fd4da0-d355-49fd-d1e4-a1ed4b63b189",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "input=torch.tensor([0.05,0.1])\n",
        "output = net(input)\n",
        "target = torch.tensor([0.01,0.99])\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "loss = criterion(output, target)\n",
        "print(loss)\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.2984, grad_fn=<MseLossBackward>)\n",
            "tensor([0.7514, 0.7729], grad_fn=<SigmoidBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IM5Q1ZwQ7-fL",
        "outputId": "1695be12-7dc5-4e96-fd4c-78a94023e1d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(loss.grad_fn)  # MSELoss\n",
        "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
        "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<MseLossBackward object at 0x7fb986a58ba8>\n",
            "<SigmoidBackward object at 0x7fb986a58d30>\n",
            "<AddBackward0 object at 0x7fb986a58ba8>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEfkua95_f85"
      },
      "source": [
        "Backward Pass and Gradients w.r.t weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocNclizk7-fN",
        "outputId": "23bebf7c-f47e-4435-bb34-b8a9a247c66c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
        "\n",
        "print('conv1.bias.grad before backward')\n",
        "print(net.fc2.weight.grad)\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print('conv1.bias.grad after backward')\n",
        "print(net.fc2.weight.grad)\n",
        "print(net.fc1.weight.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv1.bias.grad before backward\n",
            "tensor([[0., 0.],\n",
            "        [0., 0.]])\n",
            "conv1.bias.grad after backward\n",
            "tensor([[ 0.0822,  0.0827],\n",
            "        [-0.0226, -0.0227]])\n",
            "tensor([[0.0004, 0.0009],\n",
            "        [0.0005, 0.0010]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUiQ2gC3_nzv"
      },
      "source": [
        "Optimizing the weights with regards to gradients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vo7u4leA7-fQ"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# create your optimizer\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "# in your training loop:\n",
        "optimizer.zero_grad()   # zero the gradient buffers\n",
        "output = net(input) # FORWARD\n",
        "loss = criterion(output, target) # LOSS\n",
        "loss.backward() # BACKWARD\n",
        "optimizer.step()    # Does the update"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7ADvZZR7-fS"
      },
      "source": [
        ".. Note::\n",
        "\n",
        "      Observe how gradient buffers had to be manually set to zero using\n",
        "      ``optimizer.zero_grad()``. This is because gradients are accumulated\n",
        "      as explained in the `Backprop`_ section.\n",
        "\n"
      ]
    }
  ]
}