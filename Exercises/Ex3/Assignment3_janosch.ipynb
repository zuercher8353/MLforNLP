{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Task\n",
    "In this task, you would require to claasify the BBC News text into 5 classes ['business' 'entertainment' 'politics' 'sport''tech']. For this task, the code skeleton has been given and you have to write your code below #TODO parts. Comments are given with #TODO as helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing relevant libraries \n",
    "If any of the below list libraries is not installed already, then use \"pip install #library_name\" to install it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: torch==1.6.0 in /Users/janoschbaltensperger/opt/anaconda3/lib/python3.7/site-packages (1.6.0)\nRequirement already satisfied: numpy in /Users/janoschbaltensperger/opt/anaconda3/lib/python3.7/site-packages (from torch==1.6.0) (1.18.1)\nRequirement already satisfied: future in /Users/janoschbaltensperger/opt/anaconda3/lib/python3.7/site-packages (from torch==1.6.0) (0.18.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing BBC News Dataset\n",
    "Source data from public data set on BBC news articles:\n",
    "D. Greene and P. Cunningham. \"Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering\", Proc. ICML 2006. [PDF] [BibTeX].\n",
    "\n",
    "http://mlg.ucd.ie/datasets/bbc.html\n",
    "\n",
    "Cleaned up version of the Dataset is given as csv file with the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"bbc-text_train.csv\")\n",
    "data_test= pd.read_csv(\"bbc-text_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        category                                               text\n",
       "0  entertainment  farrell due to make us tv debut actor colin fa...\n",
       "1       business  china continues rapid growth china s economy h...\n",
       "2       business  ebbers  aware  of worldcom fraud former worldc...\n",
       "3  entertainment  school tribute for tv host carson more than 1 ...\n",
       "4           tech  broadband fuels online expression fast web acc..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>category</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>entertainment</td>\n      <td>farrell due to make us tv debut actor colin fa...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>business</td>\n      <td>china continues rapid growth china s economy h...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>business</td>\n      <td>ebbers  aware  of worldcom fraud former worldc...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>entertainment</td>\n      <td>school tribute for tv host carson more than 1 ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>tech</td>\n      <td>broadband fuels online expression fast web acc...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "sport            413\n",
       "business         409\n",
       "politics         334\n",
       "tech             319\n",
       "entertainment    305\n",
       "Name: category, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "data_train['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting training data into Train and validation set\n",
    "Note: Validation set is surrogate to test set and while training the network , we evaluate the model on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_df,val_x_df,train_y_df,val_y_df = train_test_split(data_train['text'],data_train['category'],test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding prediction classes/labels into integers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['business' 'entertainment' 'politics' 'sport' 'tech']\n"
     ]
    }
   ],
   "source": [
    "le.fit(train_y_df)\n",
    "print(le.classes_)\n",
    "train_y=le.transform(train_y_df)\n",
    "val_y=le.transform(val_y_df)\n",
    "test_y=le.transform(data_test['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting News text into numerical vector using count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit_transform(train_x_df)\n",
    "train_x=vectorizer.transform(train_x_df)\n",
    "val_x=vectorizer.transform(val_x_df)\n",
    "test_x=vectorizer.transform(data_test['text'])\n",
    "input_dim = len(test_x.toarray()[1]) #24295"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "24295\n"
     ]
    }
   ],
   "source": [
    "train_x.toarray()\n",
    "print(len(test_x.toarray()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ClassificationNet, self).__init__()\n",
    "        '''\n",
    "        Defining layers of neural network\n",
    "        '''\n",
    "        #TODO 1\n",
    "        # input dim = 24295 = len(test_x.toarray()[1])\n",
    "        # output dim = 5 = nr of categories)\n",
    "        self.fc1 = nn.Linear(24295, 5)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"The forward pass of the classifier\n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor. \n",
    "                x_in.shape should be (data_points, num_features)\n",
    "        Returns:\n",
    "            the resulting tensor.\n",
    "        \"\"\"\n",
    "        #TODO 2\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "net =ClassificationNet()\n",
    "\n",
    "# TODO 3 (define learning rate)\n",
    "w = 0.001\n",
    "\n",
    "\n",
    "# TODO 4 (Construct/define an optimizer object)\n",
    "#optimizer similiar \"adaptive gradient descent\" \n",
    "optimizer = optim.Adam(net.parameters(), lr=0.05)\n",
    "\n",
    "# TODO 5 (Construct an loss/criterion object)\n",
    "criterion = nn.BCELoss() #BCELoss() is suited if classifie 0 or 1\n",
    "\n",
    "#TODO 6 (define number of epochs/ number of training iteration)\n",
    "epochs= 30\n",
    "\n",
    "#converting train and validation set arrays to tensor\n",
    "train_x_tensor=torch.tensor(train_x.toarray()).float()\n",
    "train_y_tensor=torch.tensor(train_y)\n",
    "val_x_tensor=torch.tensor(val_x.toarray()).float()\n",
    "val_y_tensor=torch.tensor(val_y)\n",
    "\n",
    "\n",
    "def evaluation_metrics(predict_y,ground_truth_y):\n",
    "    '''\n",
    "    Returns accuracy and f1 score metrics for evaluation\n",
    "    '''\n",
    "    accuracy=accuracy_score(ground_truth_y,predict_y)\n",
    "    f1score=f1_score(ground_truth_y,predict_y,average='macro')\n",
    "    \n",
    "    return accuracy,f1score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/30 - Loss_train: 0.703   loss_val: 2.151   accuracy_val: 0.980 f1score_val: 0.979   \n",
      "Epoch 2/30 - Loss_train: 2.067   loss_val: 1.773   accuracy_val: 0.949 f1score_val: 0.945   \n",
      "Epoch 3/30 - Loss_train: 1.429   loss_val: 0.777   accuracy_val: 0.935 f1score_val: 0.929   \n",
      "Epoch 4/30 - Loss_train: 0.370   loss_val: 0.268   accuracy_val: 0.938 f1score_val: 0.933   \n",
      "Epoch 5/30 - Loss_train: 0.072   loss_val: 0.116   accuracy_val: 0.947 f1score_val: 0.942   \n",
      "Epoch 6/30 - Loss_train: 0.023   loss_val: 0.108   accuracy_val: 0.949 f1score_val: 0.946   \n",
      "Epoch 7/30 - Loss_train: 0.031   loss_val: 0.157   accuracy_val: 0.952 f1score_val: 0.949   \n",
      "Epoch 8/30 - Loss_train: 0.060   loss_val: 0.309   accuracy_val: 0.952 f1score_val: 0.949   \n",
      "Epoch 9/30 - Loss_train: 0.089   loss_val: 0.346   accuracy_val: 0.961 f1score_val: 0.958   \n",
      "Epoch 10/30 - Loss_train: 0.089   loss_val: 0.398   accuracy_val: 0.963 f1score_val: 0.962   \n",
      "Epoch 11/30 - Loss_train: 0.060   loss_val: 0.386   accuracy_val: 0.975 f1score_val: 0.974   \n",
      "Epoch 12/30 - Loss_train: 0.043   loss_val: 0.324   accuracy_val: 0.978 f1score_val: 0.977   \n",
      "Epoch 13/30 - Loss_train: 0.031   loss_val: 0.311   accuracy_val: 0.975 f1score_val: 0.974   \n",
      "Epoch 14/30 - Loss_train: 0.011   loss_val: 0.298   accuracy_val: 0.980 f1score_val: 0.979   \n",
      "Epoch 15/30 - Loss_train: 0.007   loss_val: 0.290   accuracy_val: 0.980 f1score_val: 0.979   \n",
      "Epoch 16/30 - Loss_train: 0.004   loss_val: 0.283   accuracy_val: 0.980 f1score_val: 0.979   \n",
      "Epoch 17/30 - Loss_train: 0.002   loss_val: 0.279   accuracy_val: 0.978 f1score_val: 0.976   \n",
      "Epoch 18/30 - Loss_train: 0.001   loss_val: 0.276   accuracy_val: 0.972 f1score_val: 0.971   \n",
      "Epoch 19/30 - Loss_train: 0.001   loss_val: 0.273   accuracy_val: 0.972 f1score_val: 0.971   \n",
      "Epoch 20/30 - Loss_train: 0.001   loss_val: 0.271   accuracy_val: 0.972 f1score_val: 0.971   \n",
      "Epoch 21/30 - Loss_train: 0.001   loss_val: 0.270   accuracy_val: 0.972 f1score_val: 0.971   \n",
      "Epoch 22/30 - Loss_train: 0.000   loss_val: 0.221   accuracy_val: 0.972 f1score_val: 0.971   \n",
      "Epoch 23/30 - Loss_train: 0.000   loss_val: 0.220   accuracy_val: 0.969 f1score_val: 0.968   \n",
      "Epoch 24/30 - Loss_train: 0.000   loss_val: 0.219   accuracy_val: 0.969 f1score_val: 0.968   \n",
      "Epoch 25/30 - Loss_train: 0.000   loss_val: 0.219   accuracy_val: 0.969 f1score_val: 0.968   \n",
      "Epoch 26/30 - Loss_train: 0.000   loss_val: 0.218   accuracy_val: 0.969 f1score_val: 0.968   \n",
      "Epoch 27/30 - Loss_train: 0.000   loss_val: 0.217   accuracy_val: 0.969 f1score_val: 0.968   \n",
      "Epoch 28/30 - Loss_train: 0.000   loss_val: 0.216   accuracy_val: 0.969 f1score_val: 0.968   \n",
      "Epoch 29/30 - Loss_train: 0.000   loss_val: 0.216   accuracy_val: 0.969 f1score_val: 0.968   \n",
      "Epoch 30/30 - Loss_train: 0.000   loss_val: 0.215   accuracy_val: 0.969 f1score_val: 0.968   \n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    \n",
    "    # the training routine is these 5 steps:\n",
    "    \n",
    "    # TODO 7 step 1. zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # TODO 8 step 2. compute the output\n",
    "    output = net(train_x_tensor)\n",
    "    \n",
    "    \n",
    "    # TODO 9 step 3. compute the loss (name this as \"loss\")\n",
    "    #given y-val (category 0-4) construct a vector(1,5) initalized to 0, then set vector[y-val] = 1\n",
    "    #add the vectors to an array \n",
    "    #transform array to tensor\n",
    "    train_y_adapted = []\n",
    "    for ele in train_y:\n",
    "        ele_res = [0,0,0,0,0]\n",
    "        ele_res[ele] = 1\n",
    "        train_y_adapted.append(ele_res) \n",
    "    \n",
    "    train_y_adapted_tensor = torch.tensor(train_y_adapted)\n",
    "    loss = criterion(output, train_y_adapted_tensor.float())\n",
    "    \n",
    "    # TODO 10 use loss to produce gradients\n",
    "    #calc gardients\n",
    "    loss.backward()\n",
    "    \n",
    "    # TODO 11 use optimizer to take gradient step\n",
    "    optimizer.step() \n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # validation set evaluation:\n",
    "        \n",
    "        # TODO 11 compute the output\n",
    "        output_val = net(val_x_tensor)\n",
    "\n",
    "        \n",
    "        # TODO 12 compute the loss (name this as \"loss_val\")\n",
    "        #given y-val (category 0-4) construct a vector(1,5) initalized to 0, then set vector[y-val] = 1\n",
    "        #add the vectors to an array \n",
    "        #transform array to tensor\n",
    "        val_y_adapted = []\n",
    "        for ele in val_y:\n",
    "            ele_res = [0,0,0,0,0]\n",
    "            ele_res[ele] = 1\n",
    "            val_y_adapted.append(ele_res) \n",
    "\n",
    "        val_y_adapted_tensor = torch.tensor(val_y_adapted)\n",
    "        loss_val = criterion(output_val, val_y_adapted_tensor.float())\n",
    "        \n",
    "        # TODO 13 compute the prediction\n",
    "        # take as prediction the category which has the hight value\n",
    "        prediction = []\n",
    "        for idx, val1 in enumerate(output_val):\n",
    "            max = -1\n",
    "            max_pos = 0\n",
    "            for idx2, ele in enumerate(val1):\n",
    "                if ele > max:\n",
    "                    max = ele\n",
    "                    max_pos = idx2\n",
    "            prediction.append(max_pos)\n",
    "            \n",
    "        \n",
    "        # TODO 14 Use the \"evaluation_metrics\" function to find accuracy and f1 score and name this as               \"accuracy\",\"f1score\")\n",
    "        (accuracy, f1score) = evaluation_metrics(val_y_tensor, prediction)\n",
    "\n",
    "        \n",
    "        \n",
    "        print('Epoch %d/%d - Loss_train: %.3f   loss_val: %.3f   accuracy_val: %.3f f1score_val: %.3f   '% \\\n",
    "            (i + 1, epochs,loss.item(),loss_val.item(),accuracy,f1score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy_test: 0.971 f1score_val: 0.971   \n"
     ]
    }
   ],
   "source": [
    "# converting test set arrays to tensor\n",
    "test_x_tensor=torch.tensor(test_x.toarray()).float()\n",
    "test_y_tensor=torch.tensor(test_y)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Test set evaluation:\n",
    "    \n",
    "    # TODO 15 compute the output\n",
    "    output_test = net(test_x_tensor)\n",
    "    \n",
    "    # TODO 16 compute the prediction\n",
    "    prediction = []\n",
    "    for idx, val1 in enumerate(output_test):\n",
    "        max = -1\n",
    "        max_pos = 0\n",
    "        for idx2, ele in enumerate(val1):\n",
    "            if ele > max:\n",
    "                max = ele\n",
    "                max_pos = idx2\n",
    "        prediction.append(max_pos)\n",
    "    \n",
    "    # TODO 17 Use the \"evaluation_metrics\" function to find accuracy and f1 score\n",
    "    (accuracy_test, f1score_test) = evaluation_metrics(test_y_tensor, prediction)\n",
    "    \n",
    "    print('Accuracy_test: %.3f f1score_val: %.3f   '% (accuracy_test,f1score_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}