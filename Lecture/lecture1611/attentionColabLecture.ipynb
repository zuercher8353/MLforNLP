{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "attentionColab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNp4x1IPyQKh",
        "outputId": "301567a5-156f-4aee-b18f-97e8e6bd4682",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgdV_BjWcOOC"
      },
      "source": [
        "## Induction of the polarity of a word from context word\n",
        "\n",
        "so far: given the embedding of a word, learn to predict its polarity\n",
        "e.g. given \"hope\" = positive, learn to predict it form the embedding\n",
        "\n",
        "### binary classification using a word context\n",
        "now: given a  word as context, learn to predict if they have the same polarity\n",
        "e.g. given \"joy\", combine it with \"hope\", predict whether the have the same polarity\n",
        "\n",
        "idea: attention would improve it over the single case approach\n",
        "\n",
        "finding: yes, if we use BERT, no if we use fasttext and a simple architecture\n",
        "BERT reaches 97% accuracy\n",
        "\n",
        "training data: pairs of word with same (1) or differenct (0) polarity\n",
        "we have many trainging data now, almost n x n, where n is the size of the lexicon (we don't combine a word with itself)\n",
        "\n",
        "### classifing pairs for \"same polarity\"\n",
        "\n",
        "* we have pos,pos pairs (neg,neg and neut,neut)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNE3occeIdyO"
      },
      "source": [
        "import json\n",
        "\n",
        "path=\"/content/gdrive/My Drive/ml20/\"\n",
        "\n",
        "with open(path+\"polex_embeddings.txt\",\"r\") as infile:\n",
        "    emb2 = json.load(infile)\n",
        "\n",
        "with open(path+\"polex_embedding_index.txt\",\"r\") as infile:\n",
        "    id_to_pol = json.load(infile)\n",
        "    \n",
        "with open(path+\"polex_embedding_words.txt\",\"r\") as infile:\n",
        "    id_to_word = json.load(infile)\n",
        "    \n",
        "with open(path+\"polex_embedding_pollist.txt\",\"r\") as infile:\n",
        "    posnegneut = json.load(infile)\n",
        "\n",
        "\n",
        "emb=list(emb2['1'])\n",
        "\n",
        "pos=posnegneut[\"1\"]\n",
        "neg=posnegneut[\"0\"]\n",
        "neut=posnegneut[\"2\"]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCtq19vTKzPq"
      },
      "source": [
        "from random import randint\n",
        "\n",
        "def getpair(data,a,b,pol,c):\n",
        "    pairs=[]\n",
        "\n",
        "    for i in range(0,len(a)-1):\n",
        "        for _ in range(0,c):\n",
        "            j=randint(0,len(b)-1)\n",
        "            if a[i] in data and b[j] in data:  # i!=j is possible, but doesn't harm\n",
        "                pairs.append((b[j],a[i],pol))\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def mappol(pol):\n",
        "    if pol==0:\n",
        "        return [1,0,0]\n",
        "    elif pol==1:\n",
        "        return [0,1,0]\n",
        "    else:\n",
        "        return [0,0,1]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSd5o_a0udfU"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "def datasplit():\n",
        "  ids=list(id_to_pol.keys())   # list of ids of words pointing to their embedding vector\n",
        "  ids=[int(i) for  i in ids]\n",
        "  ids=shuffle(ids)   \n",
        "\n",
        "  split=int(len(ids)*0.7)   # 70% to train\n",
        "    \n",
        "  train=ids[:split]\n",
        "\n",
        "  test=ids[split+1:]        \n",
        "\n",
        "  # create pairs for training and testing\n",
        "  #depth\n",
        "  d=5\n",
        "  data=train\n",
        "  pair_pos = getpair(data,pos,pos,1,d)   # 15 = number of pair per noun\n",
        "  pair_neg = getpair(data,neg,neg,0,d)\n",
        "  pair_neut = getpair(data,neut,neut,2,d)\n",
        "    \n",
        "  data=test\n",
        "  ypair_pos = getpair(data,pos,pos,1,d)  \n",
        "  ypair_neg = getpair(data,neg,neg,0,d)\n",
        "  ypair_neut = getpair(data,neut,neut,2,d)\n",
        "\n",
        "  \n",
        "  same_pol=pair_pos+pair_neg +pair_neut\n",
        "  ysame_pol=ypair_pos+ypair_neg +ypair_neut\n",
        "\n",
        "  X_test2=ysame_pol # +ydiff_pol\n",
        "  X_test3=shuffle(X_test2,random_state=0)\n",
        "\n",
        "  X_test = [[emb[a],emb[b]] for (a,b,_) in X_test3]\n",
        "  y_test =[mappol(pol) for (_,_,pol) in X_test3]\n",
        "\n",
        "  X_train2=same_pol #+diff_pol\n",
        "  X_train3=ids=shuffle(X_train2,random_state=0)                    \n",
        "\n",
        "  X =[[emb[a],emb[b]] for (a,b,_) in X_train3]\n",
        "  y =[mappol(pol) for (_,_,pol) in X_train3]\n",
        "\n",
        "  return X,y,X_test,y_test"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIYM8GU8udfc"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class attention(nn.Module):\n",
        "    def __init__(self,  fdim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.tokeys = nn.Linear(fdim,fdim)\n",
        "        self.toqueries = nn.Linear(fdim,fdim)\n",
        "        self.tovalues = nn.Linear(fdim,fdim)\n",
        "                    \n",
        "    def forward(self,x):\n",
        "\n",
        "        Q=self.toqueries(x)\n",
        "        K=self.tokeys(x)\n",
        "        V=self.tovalues(x)\n",
        "        QK = Q @ K.transpose(1,2)\n",
        "        QK_softmax = F.softmax(QK, dim=-1)\n",
        "        \n",
        "        weighted_values = QK_softmax @ V\n",
        "\n",
        "        return weighted_values"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KU2X5Xkpudfo"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class SentimentData(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.labels = labels\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):     \n",
        "        X = self.data[index]\n",
        "        y = self.labels[index]\n",
        "\n",
        "        return torch.tensor(X,dtype=torch.float32,requires_grad=True), torch.tensor(y,dtype=torch.float32)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3XVfQHi43At"
      },
      "source": [
        "class transformer3(nn.Module):\n",
        "    \n",
        "    def __init__(self, k):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attention = attention(k)\n",
        "        self.norm = nn.LayerNorm(k)\n",
        "\n",
        "        self.ff = nn.Sequential(\n",
        "          nn.Linear(k , k),\n",
        "         # nn.ReLU(),\n",
        "          nn.Linear(k, 3))\n",
        "            \n",
        "    def forward(self, x):\n",
        "                \n",
        "        x = self.attention(x)\n",
        "        x = x.unsqueeze(dim=0)\n",
        "        x=self.norm(x)   \n",
        "\n",
        "        x=self.ff(x)\n",
        "\n",
        "        x= F.max_pool2d(x,kernel_size=(2,1)) \n",
        "\n",
        "        m=nn.Softmax(dim=3)\n",
        "        x=m(x)\n",
        "        \n",
        "        return x.squeeze(dim=2)\n",
        "\n",
        "\n",
        "class transformer2(nn.Module):\n",
        "    \n",
        "    def __init__(self, k):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attention = attention(k)\n",
        "        self.norm = nn.LayerNorm(k)\n",
        "\n",
        "        self.ff = nn.Sequential(\n",
        "          nn.Linear(k, 4 * k),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(4 * k, k ),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(k , k),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(k, k),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(k, 3))\n",
        "            \n",
        "    def forward(self, x):\n",
        "                \n",
        "        x = self.attention(x)\n",
        "        x = x.unsqueeze(dim=0)\n",
        "        x=self.norm(x)   \n",
        "\n",
        "        x=self.ff(x)\n",
        "\n",
        "        x= F.max_pool2d(x,kernel_size=(2,1)) \n",
        "\n",
        "        m=nn.Softmax(dim=3)\n",
        "        x=m(x)\n",
        "        \n",
        "        return x.squeeze(dim=2)\n",
        "\n",
        "\n",
        "class transformer(nn.Module):\n",
        "    \n",
        "    def __init__(self, k):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attention = attention(k)\n",
        "        self.norm = nn.LayerNorm(k)\n",
        "        \n",
        "        self.ff = nn.Sequential(\n",
        "          nn.Linear(k, 4 * k),\n",
        "          nn.Dropout(),   # default=0.5\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(),\n",
        "          nn.Linear(4 * k, k ),\n",
        "          nn.Dropout(0.25),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(0.25),\n",
        "          nn.Linear(k , k),\n",
        "          nn.Dropout(0.25),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(0.25),\n",
        "          nn.Linear(k, k),\n",
        "          nn.Dropout(0.25),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(0.25),\n",
        "          nn.Linear(k, 3),\n",
        "          nn.Dropout(0.25))\n",
        "            \n",
        "    def forward(self, x):\n",
        "                \n",
        "        x = self.attention(x)\n",
        "        x = x.unsqueeze(dim=0)\n",
        "        x=self.norm(x)   \n",
        "\n",
        "        x=self.ff(x)\n",
        "\n",
        "        x= F.avg_pool2d(x,kernel_size=(2,1)) \n",
        "\n",
        "        m=nn.Softmax(dim=3)\n",
        "        x=m(x)\n",
        "        \n",
        "        return x.squeeze(dim=2)\n",
        "    \n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IFAK57HLaei"
      },
      "source": [
        "def training(epochs):\n",
        "  for epoch in range(epochs):\n",
        "      for input,label in  dataloader:\n",
        "          optimizer.zero_grad()       \n",
        "      \n",
        "          outputs = trans(input.to(device))   \n",
        "          label=label.to(device)\n",
        "  \n",
        "          loss = loss_func(outputs, label.unsqueeze(dim=0))\n",
        "        \n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "\n",
        "def evaluate(X_test,y_test):\n",
        "  tp,all=0,0\n",
        "\n",
        "  for input,label in zip(X_test,y_test):\n",
        "      input=torch.tensor(input,dtype=torch.float)\n",
        "    \n",
        "      input=input.to(device)\n",
        "      output = trans(input.unsqueeze(0))\n",
        "      predict=torch.argmax(output)\n",
        "      predict=predict.cpu()\n",
        " #   print(predict.detach().numpy(), np.argmax(label))\n",
        "  \n",
        "      if predict.detach().numpy() == np.argmax(label):\n",
        "          tp+=1\n",
        "      all+=1\n",
        "  return tp/all\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_g7QYn8fJrPh",
        "outputId": "1f076022-6ed5-4148-c654-5cac8a8ff4a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# run it\n",
        "\n",
        "batch_size=50\n",
        "epochs=1\n",
        "\n",
        "acc=0\n",
        "loss_func = torch.nn.MSELoss()\n",
        "\n",
        "folds=5  # crossvalidation\n",
        "\n",
        "for i in range(folds):\n",
        "  print(\"fold\",i+1)\n",
        "  X,y,X_test,y_test = datasplit()\n",
        "\n",
        "  data=SentimentData(X,y)\n",
        "  dataloader=DataLoader(dataset=data,batch_size=batch_size,shuffle=True)\n",
        "  trans=transformer(300)\n",
        "  trans.to(device)\n",
        "  trans.train()    # do apply dropout\n",
        "\n",
        "  optimizer = torch.optim.Adam(trans.parameters(), weight_decay=1e-5, lr=1e-4)\n",
        "\n",
        "  print(\"learning\")\n",
        "  training(epochs)   \n",
        "  print(\"testing\")\n",
        "  trans.eval()     # do not apply dropout\n",
        "  acc2=evaluate(X_test,y_test)   \n",
        "  print(acc2)\n",
        "  acc+=acc2\n",
        "\n",
        "print(\"mean accuracy=\", acc/folds)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fold 1\n",
            "learning\n",
            "testing\n",
            "0.8958875585632483\n",
            "fold 2\n",
            "learning\n",
            "testing\n",
            "0.8879581151832461\n",
            "fold 3\n",
            "learning\n",
            "testing\n",
            "0.8750634840020315\n",
            "fold 4\n",
            "learning\n",
            "testing\n",
            "0.8722724853645556\n",
            "fold 5\n",
            "learning\n",
            "testing\n",
            "0.8915478615071283\n",
            "mean accuracy= 0.8845459009240418\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}