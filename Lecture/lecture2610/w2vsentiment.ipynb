{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the polarity of a polar word\n",
    "\n",
    "* use a polarity lexicon with positive and negative words\n",
    "* take half of it to train a feedforward net and have to test\n",
    "* use the embedding of a word as input representation\n",
    "\n",
    "\n",
    "* a real setting would be to predict 3 classes: neutral, positive, negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from our textbook\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def load_emb_from_file(filepath):\n",
    "    \"\"\"\n",
    "    Load  embeddings \n",
    "    \n",
    "    Args:\n",
    "        filepath (str): path to the embeddings file \n",
    "    Returns:\n",
    "        word_to_index (dict), embeddings (numpy.ndarary)\n",
    "    \"\"\"\n",
    "  \n",
    "\n",
    "    word_to_index = {}\n",
    "    embeddings = []\n",
    "    with open(filepath, \"r\") as fp:\n",
    "        for index, line in enumerate(fp):\n",
    "            line = line.split(\" \") # each line: word num1 num2 ...\n",
    "            word_to_index[line[0]] = index # word = line[0] \n",
    "            embedding_i = np.array([float(val) for val in line[1:]])\n",
    "            embeddings.append(embedding_i)\n",
    "    return word_to_index, np.stack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "widx,emb=load_emb_from_file(\"/home/klenner/applications/gensim-0.13.4/vectors.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7737,\n",
       " array([-0.09773 , -0.073794, -0.052194, -0.499479, -0.319778,  0.157181,\n",
       "         0.269456,  0.610306,  0.09075 ,  0.361076, -0.495249,  0.22715 ,\n",
       "         0.155042,  0.316588,  0.479143, -0.076718, -0.102654, -0.714726,\n",
       "         0.155304, -0.565182, -0.829133,  0.245589,  0.778199,  0.019168,\n",
       "        -0.461478, -0.024124, -0.552235,  0.386096, -0.191506,  0.622527,\n",
       "        -0.096904,  0.669733,  0.477398,  0.317568,  0.165939, -0.138258,\n",
       "        -0.785988,  0.142101,  0.549931, -0.345421, -0.532162,  0.016093,\n",
       "        -0.057987,  0.13559 ,  0.214938,  0.078067,  0.113646, -0.10188 ,\n",
       "        -0.036438, -0.085324, -0.193315,  0.511736,  0.142473, -0.223453,\n",
       "        -0.069988,  0.621426, -0.454478, -0.097701,  0.174274, -0.199256,\n",
       "        -0.343834,  0.010616,  0.294296,  0.183556,  0.301568, -0.03067 ,\n",
       "         0.723429, -0.305649,  0.408184, -0.204259, -0.268337, -0.153939,\n",
       "         0.613482,  0.185519, -0.992607, -0.028342, -0.106007,  0.286124,\n",
       "        -0.16511 , -0.546778,  0.079181, -0.098937,  0.487011, -0.437198,\n",
       "        -0.404379,  0.609281, -0.041873, -0.486136, -0.37829 , -0.005116,\n",
       "         0.024344, -0.336522, -0.286049,  0.328317, -0.526915, -0.596616,\n",
       "         0.624931,  0.252005, -0.23362 , -0.192212,  0.240147,  0.075763,\n",
       "        -0.322579, -0.030123, -0.360747, -0.392913,  0.246061, -0.6689  ,\n",
       "        -0.152102, -0.240757,  0.263455, -0.535474,  0.633451, -0.406007,\n",
       "        -0.676362, -0.160122,  0.538765,  0.250412,  0.735716, -0.499545,\n",
       "        -0.097524,  0.614795,  0.191462, -0.269911, -0.103386, -0.337424,\n",
       "         0.022851, -0.199089, -0.732735,  0.376984,  0.350178, -0.24672 ,\n",
       "        -0.234722, -0.356625,  0.081113,  0.270822,  0.30274 , -0.306035,\n",
       "         0.670309, -0.205823,  0.33372 , -0.040492, -0.325553, -0.103974,\n",
       "         0.133832,  0.447073,  0.248699, -0.427352,  0.050569, -0.364028,\n",
       "         0.261834,  0.481898, -0.131316, -0.313704,  0.312779, -0.539352,\n",
       "         0.274345, -0.16616 , -0.192087,  0.084676, -0.147764, -0.123606,\n",
       "        -0.005754, -0.365277, -0.617185,  0.837148,  0.169989, -0.400005,\n",
       "         0.180207,  0.107414,  0.001696, -0.217325, -0.323425,  0.102439,\n",
       "        -0.029111, -0.164353,  0.322857, -0.244578,  0.103721,  0.289376,\n",
       "         0.315191,  0.79027 , -0.089812, -0.042278, -0.166963, -0.027043,\n",
       "         0.84523 , -0.589813,  0.04701 ,  0.23449 , -0.345261, -0.459356,\n",
       "         0.408981,  0.590454, -0.275471, -0.075367,  0.427641, -0.389057,\n",
       "        -0.200599,  0.35581 ,  0.836038, -0.597761,  0.248851,  0.276391,\n",
       "         0.810452, -0.122332, -0.14645 , -0.117553,  0.179321,  0.752825,\n",
       "         0.051876,  0.621365,  0.183632, -0.445991, -0.372842, -0.097624,\n",
       "         0.146068,  0.283701, -0.76112 , -0.599151,  0.149766,  0.231151,\n",
       "        -0.152559,  0.246644,  0.023762, -0.19265 , -0.427834, -0.202116,\n",
       "        -0.283   ,  0.14673 ,  0.654743,  0.105587,  0.268066,  0.043492,\n",
       "         0.335161, -0.299947, -0.059881,  0.006791,  0.094724,  0.960216,\n",
       "         0.172874,  0.290715,  0.23774 ,  0.003292, -0.35023 , -0.004412,\n",
       "         0.145133,  0.431392,  0.196874, -0.721554,  0.049287,  0.620848,\n",
       "        -0.791483,  0.238536,  0.225073, -0.626802, -0.427469,  0.199645,\n",
       "        -0.276553,  0.734225,  0.368158,  0.015961,  0.531589, -0.339651,\n",
       "         0.497838, -0.384881, -0.075164,  0.418874, -0.128546, -0.297902,\n",
       "        -0.329299, -0.511599,  0.47978 ,  0.153017, -0.198489, -0.08402 ,\n",
       "         0.291462, -0.723323, -0.023982, -0.132432,  0.15929 , -0.710042,\n",
       "         0.527364, -0.175065,  0.22377 , -0.197612,  0.227814, -0.060936,\n",
       "         0.597614,  0.746139, -0.18988 ,  0.240411, -0.034729,  0.640131,\n",
       "         0.133315,  0.125575,  0.053594, -0.338249, -0.480272, -0.477948]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# access the embedding of a word via its index\n",
    "\n",
    "index=widx['freude']\n",
    "word_embedding=emb[index]\n",
    "index, word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.619, 1238, 762)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# return lemma,polarity pairs\n",
    "polex=pd.read_csv(\"/home/klenner/python/python/data_lexicon/polex_de_clean\",header=None,\n",
    "                  index_col=[0,1],usecols=[0,1],names=['lemma', 'pol'])\n",
    "\n",
    "ids=[]     # gather all embedding indices\n",
    "index={}   # map index to polarity\n",
    "\n",
    "for (l,p),_ in polex.iterrows():  # (lemma,polarity) pairs\n",
    "    try:\n",
    "        id=widx[l.lower()]  # lowercase, since word2vec version requests this\n",
    "        if p=='POS':\n",
    "            index[id]=1     # id is the word2vec index of lemma l\n",
    "            ids.append(id)  # all ids for data split below\n",
    "        elif p=='NEG':\n",
    "            index[id]=0\n",
    "            ids.append(id)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "np.random.shuffle(ids)                    # random modifies ids directly\n",
    "noun=[(id,index[id]) for id in ids]       # create input pairs: (word2vecID,polarity)\n",
    "\n",
    "# split in train and test\n",
    "train=noun[:2000]\n",
    "test=noun[2001:]\n",
    "\n",
    "# what is the baseline in a majority voting setting\n",
    "pos=[1 for l,p in train if p == 1]\n",
    "neg=[1 for l,p in train if p == 0]\n",
    "\n",
    "a,b=len(pos),len(neg)   # well negative words are the majority class\n",
    "\n",
    "baseline = b/(a+b)\n",
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6081025217031831"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# would it work for the test set?\n",
    "\n",
    "pos=[1 for l,p in test if p == 1]\n",
    "neg=[1 for l,p in test if p == 0]\n",
    "\n",
    "a,b=len(pos),len(neg)   # well negative words are the majority class\n",
    "\n",
    "b/(a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "853624"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emb)                       # vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "embedding_dim=300\n",
    "\n",
    "class Net(nn.Module):   \n",
    "    def __init__(self,):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(embedding_dim,1)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))  \n",
    "        return x\n",
    "    \n",
    "net=Net()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "loss_func = nn.BCELoss()\n",
    "\n",
    "for epoch in range(2):\n",
    "    for id, label in  train:\n",
    "        input=torch.tensor(emb[id], dtype=torch.float32, requires_grad=True)\n",
    "        optimizer.zero_grad()       \n",
    "        outputs = net(input)\n",
    "        label=torch.tensor([label],dtype=torch.float)\n",
    "        loss = loss_func(outputs, label )\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8515915667631253"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def step(x):\n",
    "    if x < 0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "tp=0\n",
    "for wid ,label in test:\n",
    "    input=emb[wid]\n",
    "    input=torch.tensor(input, dtype=torch.float32)  \n",
    "    output = net(input)\n",
    "    \n",
    "    if step(output)==label:\n",
    "        tp+=1\n",
    "\n",
    "acc=tp/len(test)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "Y = np.array([[2,-1],[-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(X)\n",
    "\n",
    "distances, indices = nbrs.kneighbors(Y)\n",
    "\n",
    "distances,indices         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "indices: e.g. for the first element of the Y, 4 from X is the closest neighbor\n",
    "\n",
    "array([[4],\n",
    "        [1],\n",
    "        [2],\n",
    "        [3],\n",
    "        [4],\n",
    "        [5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7329474989665151\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "X_train = [emb[index] for (index,_) in train]\n",
    "pols = [pol for (_,pol) in train]\n",
    "neigh = NearestNeighbors(n_neighbors=1)\n",
    "neigh.fit(X_train) \n",
    "\n",
    "tp=0\n",
    "all=0\n",
    "for (e_index,label) in test:\n",
    "    embedding=emb[e_index]\n",
    "    _,nn_index=neigh.kneighbors(embedding.reshape(1, -1))\n",
    "    train_index=nn_index[0][0]\n",
    "    knn_label=pols[train_index]\n",
    "    if label==knn_label:\n",
    "        tp+=1\n",
    "    all+=1\n",
    "\n",
    "    \n",
    "print(tp/all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
