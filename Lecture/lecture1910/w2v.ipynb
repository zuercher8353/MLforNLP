{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A very simple implementation of skip-gram in pytorch\n",
    "\n",
    "see https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'he is a king',\n",
    "    'she is a queen',\n",
    "    'he is a man',\n",
    "    'she is a woman',\n",
    "    'warsaw is poland capital',\n",
    "    'berlin is germany capital',\n",
    "    'paris is france capital',   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(corpus):\n",
    "    tokens = [x.split() for x in corpus]   # split sentence-wise\n",
    "    return tokens\n",
    "\n",
    "tokenized_corpus = tokenize_corpus(corpus)\n",
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocabulary (the word types of the model/from the corpus)\n",
    "\n",
    "vocabulary = []\n",
    "for sentence in tokenized_corpus:\n",
    "    for token in sentence:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "\n",
    "# each word gets an index position\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}  \n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "print(vocabulary,vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look two words to the left and to the right\n",
    "window_size = 2\n",
    "\n",
    "# create pairs: (center word,context word): task = predict context word\n",
    "idx_pairs = []\n",
    "# for each sentence\n",
    "for sentence in tokenized_corpus:\n",
    "    indices = [word2idx[word] for word in sentence] # map words to there indices wrt. vocabulary\n",
    "    # for each word threated as center word\n",
    "    for center_word_pos in range(len(indices)):\n",
    "        # for each window position\n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context_word_pos = center_word_pos + w   # index of the context word\n",
    "            # make soure not jump out sentence\n",
    "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                continue\n",
    "            context_word_idx = indices[context_word_pos]  # get the word index wrt. vocabulary\n",
    "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array\n",
    "print(idx_pairs)\n",
    "\n",
    "# [[ 0  1]         means: 0 of vocab  and 1 of vocab = he and is \n",
    "#  [ 0  2] .....]  he and a ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a one-hot vector: only the index of the current word is set to 1, rest is 0\n",
    "\n",
    "def get_input_layer(word_idx):\n",
    "    x = torch.zeros(vocabulary_size).float()\n",
    "    x[word_idx] = 1.0 \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dims = 5\n",
    "\n",
    "W1 = Variable(torch.randn(embedding_dims, vocabulary_size).float(), requires_grad=True)\n",
    "W2 = Variable(torch.randn(vocabulary_size, embedding_dims).float(), requires_grad=True)\n",
    "\n",
    "#initrange = 0.5\n",
    "#W1.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "i=0\n",
    "for epo in range(num_epochs):\n",
    "    loss_val = 0\n",
    "    \n",
    "    for data, target in idx_pairs: # data is center word position\n",
    "        x = Variable(get_input_layer(data)).float()  # get one-hot vector of center word\n",
    "        y_true = Variable(torch.from_numpy(np.array([target])).long()) # determine the context word index\n",
    "\n",
    "        z1 = torch.matmul(W1, x)   # access embeding vector of center word\n",
    "        z2 = torch.matmul(W2, z1)  # get weighted output for all context words\n",
    "     \n",
    "    \n",
    "        # turn weighted output into probabilities\n",
    "        log_softmax = F.log_softmax(z2,  dim=0)  # remove log and the sum is 1\n",
    "       \n",
    "        # loss is just the -neg log likelihood value of the real (ie. seen) context word (via its position)\n",
    "        loss = F.nll_loss(log_softmax.view(1,-1), y_true,reduction='mean')  # view produces a tensor with a single array\n",
    "        # y_true is the index position at which we find the loss given the log_softmax array   \n",
    "    \n",
    "        if i== -1:\n",
    "            print(y_true,\"\\t= \\tindex\\n\",loss,\"\\t= \\tloss\\n\",log_softmax.view(1,-1),\"\\t=\\toutput softmax\")\n",
    "            break\n",
    "        i+=1\n",
    "\n",
    "        loss_val += loss.data.item()   # just for output\n",
    "        loss.backward()\n",
    "    #    print(\"grad\",W1.grad.data[1])\n",
    "        W1.data -= learning_rate * W1.grad.data\n",
    "        W2.data -= learning_rate * W2.grad.data\n",
    "        \n",
    "        W1.grad.data.zero_()\n",
    "        W2.grad.data.zero_()\n",
    "        \n",
    " \n",
    " #   if epo % 10 == 0:    \n",
    " #       print(f'Loss at epo {epo}: {loss_val/len(idx_pairs)}')        \n",
    "#print(W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explanation of output (set i==1)\n",
    "\n",
    " tensor([2]) \t= \tindex\n",
    " \n",
    " tensor(4.8778, grad_fn=<NllLossBackward>) \t= \tloss is -val at index position index (2, first line)\n",
    " \n",
    " tensor([[ -4.6247,  -2.6985,  ******-4.8778*****,  -4.9672,  -6.2686,  -0.9419, -11.5144,\n",
    "          -8.4150,  -3.2777,  -6.4923,  -2.1065,  -8.1870,  -1.2819,  -2.5667,\n",
    "          -7.0781]], grad_fn=<ViewBackward>) \t=\toutput softmax\n",
    "    \n",
    " note: the values are different at each run, but the mapping is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the words for similarity via the dot product\n",
    "\n",
    "def similarity(v,u):\n",
    "  return torch.dot(v,u)/(torch.norm(v)*torch.norm(u))\n",
    "\n",
    "s1=similarity(W2[word2idx[\"she\"]], W2[word2idx[\"king\"]]) \n",
    "\n",
    "s2=similarity(W2[word2idx[\"she\"]], W2[word2idx[\"queen\"]]) \n",
    "\n",
    "s1,s2\n",
    "\n",
    "# (tensor(-0.2058, grad_fn=<DivBackward0>),\n",
    "# tensor(0.4774, grad_fn=<DivBackward0>))\n",
    "#\n",
    "# ie. she is more similar to queen than to king\n",
    "\n",
    "# note: the output is not stable, the example is too much a toy example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
