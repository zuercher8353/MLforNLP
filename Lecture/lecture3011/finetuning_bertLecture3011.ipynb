{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYbDr24bfuPG"
   },
   "source": [
    "# Finetuning BERT\n",
    "\n",
    "\n",
    "topics of this notebook:\n",
    "\n",
    "* learning to predict some label for sentences\n",
    "* predicting a masked word within a sentence\n",
    "* modifyinig a pretrained model for domain adaptation: our lexicon induction problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2jH41e7AfpYZ"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3aeef50c6911>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bIAzAxbnpfOs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.0.0-py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /Users/janoschbaltensperger/opt/anaconda3/lib/python3.7/site-packages (from transformers) (20.1)\n",
      "Requirement already satisfied: filelock in /Users/janoschbaltensperger/opt/anaconda3/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: numpy in /Users/janoschbaltensperger/opt/anaconda3/lib/python3.7/site-packages (from transformers) (1.18.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/janoschbaltensperger/opt/anaconda3/lib/python3.7/site-packages (from transformers) (4.42.1)\n",
      "Requirement already satisfied: requests in /Users/janoschbaltensperger/opt/anaconda3/lib/python3.7/site-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: six in /Users/janoschbaltensperger/opt/anaconda3/lib/python3.7/site-packages (from packaging->transformers) (1.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/janoschbaltensperger/opt/anaconda3/lib/python3.7/site-packages (from packaging->transformers) (2.4.6)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2020.11.13-cp37-cp37m-macosx_10_9_x86_64.whl (284 kB)\n",
      "\u001b[K     |████████████████████████████████| 284 kB 7.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/janoschbaltensperger/opt/anaconda3/lib/python3.7/site-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/janoschbaltensperger/opt/anaconda3/lib/python3.7/site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/janoschbaltensperger/opt/anaconda3/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/janoschbaltensperger/opt/anaconda3/lib/python3.7/site-packages (from requests->transformers) (2.8)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "\u001b[K     |████████████████████████████████| 883 kB 6.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /Users/janoschbaltensperger/opt/anaconda3/lib/python3.7/site-packages (from packaging->transformers) (1.14.0)\n",
      "Requirement already satisfied: click in /Users/janoschbaltensperger/opt/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: joblib in /Users/janoschbaltensperger/opt/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/janoschbaltensperger/opt/anaconda3/lib/python3.7/site-packages (from transformers) (4.42.1)\n",
      "Collecting tokenizers==0.9.4\n",
      "  Downloading tokenizers-0.9.4-cp37-cp37m-macosx_10_11_x86_64.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 10.2 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=f0b044263ab6cd407c2ed5a3138a641cfa88b0f3c71497ffa8327d3cc1d12ba3\n",
      "  Stored in directory: /Users/janoschbaltensperger/Library/Caches/pip/wheels/69/09/d1/bf058f7d6fa0ecba2ce7c66be3b8d012beb4bf61a6e0c101c0\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: regex, tokenizers, sacremoses, transformers\n",
      "Successfully installed regex-2020.11.13 sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_khfPJYK0nTj"
   },
   "source": [
    "# Transfer learning with BERT\n",
    "\n",
    "Transfer learing in general:\n",
    "\n",
    "* a model trained on one task is retrained for a different one\n",
    "\n",
    "* special case: BERT as a context-aware language model is retrained in a sentiment setting\n",
    "\n",
    "* learning: adopt the weights \n",
    "  * of the classifier head on top of BERT\n",
    "  * of some/all BERT layers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* we use BertForSequenceClassification\n",
    "* documentation\n",
    "  * Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n",
    "  * we get as a result: the loss and the logits of the batch elements\n",
    "  * logits: $logit=ln(\\frac{z_i}{(1-z_i)})$ where $z_i$ is some value\n",
    "* we do not need a loss function (but can use), since the Bert model already gives us the loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdlcEhvQH_8x"
   },
   "outputs": [],
   "source": [
    "a = torch.rand(5)\n",
    "logit_a=torch.logit(a, eps=1e-6)\n",
    "\n",
    "a, logit_a,  torch.log(a[0]/(1-a[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q62ZtE2hru7n"
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "MODEL_NAME = \"bert-base-german-cased\"\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "model.train() # we are in training model, ie. learn\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# torch.set_grad_enabled(False) # torch vectors without grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bIWcIOehwVaE"
   },
   "outputs": [],
   "source": [
    "# tokenize some text, provide labels\n",
    "\n",
    "import torch\n",
    "\n",
    "text_batch=[\"Der Minister lügt\",\"Der Minister ist ein netter Mensch\"]\n",
    "encoding = tokenizer(text_batch, return_tensors='pt', padding=True)\n",
    "\n",
    "input_ids = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-5, lr=1e-4)\n",
    "\n",
    "labels = torch.tensor([0,1]).unsqueeze(0)  # e.g. first sentence is negative followed by a positive one (arbitrary coding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xAcl_aMTJ7Ek"
   },
   "outputs": [],
   "source": [
    "# output\n",
    "outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "outputs[1]  # prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FCXqCH0xykyu"
   },
   "outputs": [],
   "source": [
    "# apply the model, learn\n",
    "\n",
    "outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "loss=outputs[0]\n",
    "\n",
    "print(\"prediction sentence 1\",outputs[1][0])\n",
    "print(\"\\nprediction sentence 2\",outputs[1][1])\n",
    "      \n",
    "print(\"\\npredicted class is  argument with maximum value: sentence 1\",torch.argmax(outputs[1][0]),\"     real class\",labels[0][0])\n",
    "print(\"\\npredicted class is  argument with maximum value: sentence 2\",torch.argmax(outputs[1][1]),\"     real class\",labels[0][1])\n",
    "\n",
    "optimizer.zero_grad()       \n",
    "\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Unl34JdaaPAl"
   },
   "source": [
    "if we apply the model several times (manually by crtl return of the cell), we quickly learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sOXfnqHL4DmS"
   },
   "outputs": [],
   "source": [
    "# if we do not want to alter the pretrained weights, but only the top layer (the classification head)\n",
    "\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_rQy_wZ3XoW"
   },
   "source": [
    "# Predict Masked Words\n",
    "\n",
    "let's predict a masked word (lets do on a very low level)\n",
    "\n",
    "https://demo.allennlp.org/reading-comprehension\n",
    "https://demo.allennlp.org/masked-lm?text=The%20doctor%20ran%20to%20the%20emergency%20room%20to%20see%20%5BMASK%5D%20patient.\n",
    "\n",
    "demonstrates the gender bias of bert. We now implement our own masked model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99yL7iNC3p3q"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "MODEL_NAME = \"bert-base-german-cased\"\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "text = '[CLS] Ich kaufe mir einen [MASK] . [SEP]'\n",
    "\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Create the segments tensors.\n",
    "segments_ids = [0] * len(tokenized_text)\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertForMaskedLM.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X7J-619WMavD"
   },
   "outputs": [],
   "source": [
    "# now we produces on output for each token including the mask\n",
    "\n",
    "prediction=model(tokens_tensor)\n",
    "prediction,prediction[0].size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xP_C_3FbNq2g"
   },
   "outputs": [],
   "source": [
    "# we take output at the [MASK] index, which is it?\n",
    "# not necessarily 6: text = '[CLS] Ich kaufe mir einen [MASK] . [SEP]'\n",
    "\n",
    "print(tokenized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTdUWPJbOLbd"
   },
   "outputs": [],
   "source": [
    "masked_index=6  # starting with 0, from output of cell above\n",
    "\n",
    "predicted_index = torch.argmax(prediction[0][0][masked_index]).item()   # gets BERT id of embedding\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0] # gets word token\n",
    "predicted_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hdLKpC8W3zcq"
   },
   "outputs": [],
   "source": [
    "# we'll doit in more compact way now\n",
    "\n",
    "def bertify(input):\n",
    "\n",
    "  tokenized = tokenizer(input,return_tensors='pt') \n",
    "\n",
    "  input_ids=tokenized['input_ids']\n",
    "  #segment_ids=tokenized['token_type_ids']\n",
    "  #input_mask=tokenized['attention_mask']\n",
    "  return input_ids \n",
    "\n",
    "input = '[CLS] Angela Merkel kritisierte Russland wegen dem Terrorismus - sie ist also [MASK] Terrorismus . [SEP]'\n",
    "#input = '[CLS] Er sagte dass Angela Merkel Russland wegen dem Terrorismus lobte , sie ist also [MASK] Terrorismus . [SEP]'\n",
    "\n",
    "inputstr=input.split(\" \")\n",
    "\n",
    "masked_index= [i for i in range(0,len(inputstr)) if inputstr[i]=='[MASK]'][0]+1\n",
    "\n",
    "input_ids=bertify(input)\n",
    "\n",
    "with torch.no_grad():\n",
    "  predictions=model(input_ids)\n",
    "\n",
    "print(predictions[0].size(),masked_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q8L52zOf33TL"
   },
   "outputs": [],
   "source": [
    "\n",
    "predicted_index = torch.argmax(predictions[0][0][masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "predictions[0]\n",
    "print(predicted_token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eWnPtsY3ulZ"
   },
   "source": [
    "# Lexicon induction with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kylL4San5NgE"
   },
   "outputs": [],
   "source": [
    "path=\"/content/gdrive/My Drive/ml20/\"\n",
    "\n",
    "\n",
    "with open(path+\"all_pairs\", \"r\") as file:\n",
    "    pairs = eval(file.readline())    # read one line with array of triples: [(word1,word2,polarity),...]    \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "28f_sUHMSXSS"
   },
   "outputs": [],
   "source": [
    "pairs[100]  # index 100 a positive pair, because \"Mitarbeitende\" was tagged as positiv in the underlying text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gLssMpk65rF-"
   },
   "outputs": [],
   "source": [
    "def datasplit():\n",
    "    \"\"\"\n",
    "    output: X = pairs of tokenizer arrays, each comprising input_ids,attention_mask and sequenze_ids of two words\n",
    "    output: y = the true labels (0 for neg, 1 for pos, 2 for neut)\n",
    "    X_test, y_test accordingly\n",
    "    \"\"\"\n",
    "    l=len(pairs)\n",
    "    split=int(l*0.66)  # 2/3 for training\n",
    "\n",
    "    train=pairs[:split]\n",
    "    test=pairs[split+1:]\n",
    "\n",
    "    # BERT tokenize the pairs, concatenate their input_ids \n",
    "    X_test=[tokenizer(a+b,return_tensors='pt', padding=\"max_length\",max_length=9, truncation=True) for (a,b,_) in test]\n",
    "    y_test =[pol for (_,_,pol) in test]\n",
    "\n",
    "    X =[tokenizer(a+b,return_tensors='pt', padding=\"max_length\",max_length=9, truncation=True) for (a,b,_) in train]\n",
    "    y =[pol for (_,_,pol) in train]\n",
    "\n",
    "    return X,y,X_test,y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cJCL_BUxCiu_"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch \n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "def training(epochs,X,y):\n",
    "  for epoch in range(epochs):\n",
    "      for encoding,labels in  zip(X,y):  # no dataloader used this time\n",
    "          optimizer.zero_grad()       \n",
    "\n",
    "          labels = torch.tensor(labels).unsqueeze(0)  \n",
    "\n",
    "          attention_mask = encoding['attention_mask']\n",
    "          input_mask = encoding['input_ids']\n",
    "\n",
    "          outputs = model(input_mask.to(device), attention_mask=attention_mask.to(device), labels=labels.to(device)) \n",
    "          loss=outputs[0]\n",
    "         \n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          \n",
    "       \n",
    "# divides a by b, returns 0 if b=0\n",
    "def evalit(a,b):  \n",
    "    if b==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return a/b\n",
    "  \n",
    "\n",
    "def evaluate(X_test,y_test):\n",
    "\n",
    "  tp=0\n",
    "  foundlist,tplist,allist=[0,0,0],[0,0,0],[0,0,0]    # each list manages frequencies of polarities; polarity is the list index\n",
    "                                                     # e.g. tplist[2]+=1 is used to increment (if a true positive was found of a neutral (2) pair)\n",
    "    \n",
    "  for encoding,truelabel in  zip(X,y):\n",
    "\n",
    "          attention_mask = encoding['attention_mask']\n",
    "          input_mask = encoding['input_ids']\n",
    "\n",
    "          outputs = model(input_mask.to(device), attention_mask=attention_mask.to(device))\n",
    "         \n",
    "          predict=torch.argmax(outputs[0])\n",
    "          #predict=predict.cpu()\n",
    "          predict=predict.detach().numpy() \n",
    "\n",
    "          foundlist[predict]+=1     # increase by one the dimension which represents the polarity prediction of the system\n",
    "          allist[truelabel]+=1      # increase by one the dimension which represents the true polarity (label)      \n",
    "\n",
    "          if predict == truelabel:\n",
    "            tp+=1\n",
    "            tplist[predict]+=1\n",
    "                  \n",
    "  reclist=[evalit(tplist[i],allist[i]) for i in range(0,3)]      # determine recall for neg,pos,neut\n",
    "  preclist=[evalit(tplist[i],foundlist[i]) for i in range(0,3)]\n",
    "  flist=[evalit(2*reclist[i]*preclist[i],reclist[i]+preclist[i]) for i in range(0,3)]\n",
    "\n",
    "  print(\"rec: neg,neut,pos\",reclist)\n",
    "  print(\"prec: neg,neut,pos\",preclist)\n",
    "  print(\"f: neg,neut,pos\",flist)\n",
    "\n",
    "  return tp/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MTpnBHWHDQna"
   },
   "outputs": [],
   "source": [
    "epochs=2\n",
    "acc=0\n",
    "folds=3\n",
    "\n",
    "for i in range(folds):\n",
    "  print(\"fold\",i+1)\n",
    "  X,y,X_test,y_test= datasplit()  \n",
    "\n",
    "  model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3)\n",
    "  \n",
    "  # do not touch the BERT weights, train only the classifier head \n",
    "  for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "  model.to(device)\n",
    "\n",
    "  optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-5, lr=1e-4)\n",
    "\n",
    "  model.train()\n",
    "  print(\"learning\")\n",
    "  training(epochs,X,y)   \n",
    "\n",
    "  model.eval()     \n",
    "  print(\"testing\")\n",
    " \n",
    "  fold_acc=evaluate(X_test,y_test)   \n",
    "  print(\"acc:\",fold_acc)\n",
    "  acc+=fold_acc\n",
    "\n",
    "print(\"mean accuracy=\", acc/folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KN5z9zFfaqTz"
   },
   "source": [
    "# Normalisierung\n",
    "\n",
    "* batch normalization\n",
    "  * the batch gets normalized, dimension-wise\n",
    "* layer normalization\n",
    "  * the whole layer gets normalized, over all dimentions\n",
    "\n",
    "both use the **z transformation**: $\\frac{x_i-\\mu}{\\sigma}$ where $\\mu$ = mean and $\\sigma$ = standard deviation\n",
    "\n",
    "it normalizes to a distribution with $\\mu=0$ and $\\sigma=1$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "pSOO2Ut5YA7Q"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-484f38b461cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNorm1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# batch normalization\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "m = nn.BatchNorm1d(3)  \n",
    "input = torch.tensor([[2,4,6],[4,2,1],[3,2,2]],dtype=torch.float)\n",
    "output = m(input)\n",
    "col1=output[:,0].detach().numpy()\n",
    "\n",
    "print(output)\n",
    "\n",
    "xmean=np.mean(col1)\n",
    "xvar=np.var(col1)\n",
    "\n",
    "print(\"input\",input,\"\\n\")\n",
    "print(\"first column\",col1,\"\\n\")\n",
    "print(\"mean, standard deviation\", xmean,\",\",np.sqrt(xvar))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LcMynanLa1vr"
   },
   "outputs": [],
   "source": [
    "# layer normalization: the mean of the whole layer is 0, ...\n",
    "\n",
    "m = nn.LayerNorm([3,3])\n",
    "\n",
    "normalized=m(input)\n",
    "\n",
    "# the sum (here = mean) for each vector (= all layers)\n",
    "layer_sum=sum(normalized)\n",
    "\n",
    "# the \n",
    "layer_mean=sum(layer_sum)\n",
    "print(\"normalized:\",normalized,\"\\n\")\n",
    "print(\"layer mean:\",layer_sum,\"\\n\")\n",
    "print(\"mean of whole layer:\", layer_mean)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "finetuning_bert.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
