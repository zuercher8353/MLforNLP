{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### topics\n",
    "\n",
    "* cross validation over the whole data set\n",
    "* t-test on the basis of cross validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the fasttext embeddings \n",
    "\n",
    "#import torch\n",
    "import numpy as np\n",
    "\n",
    "def load_emb_from_file(filepath):\n",
    "\n",
    "    word_to_index = {}\n",
    "    embeddings = []\n",
    "    with open(filepath, \"r\") as fp:\n",
    "        for index, line in enumerate(fp):\n",
    "            line = line.split(\" \") # each line: word num1 num2 ...\n",
    "            word_to_index[line[0]] = index # word = line[0] \n",
    "            embedding_i = np.array([float(val) for val in line[1:]])\n",
    "            embeddings.append(embedding_i)\n",
    "    return word_to_index, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/klenner/Lehre/ml20/cc.de.300.vec'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a82f3463a565>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# index, array of word embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mwidx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_emb_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/klenner/Lehre/ml20/cc.de.300.vec\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-05cc59f47f58>\u001b[0m in \u001b[0;36mload_emb_from_file\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mword_to_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# each line: word num1 num2 ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/klenner/Lehre/ml20/cc.de.300.vec'"
     ]
    }
   ],
   "source": [
    "# index, array of word embeddings\n",
    "\n",
    "widx,emb=load_emb_from_file(\"/home/klenner/Lehre/ml20/cc.de.300.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# data of learning task: polarity lexicon for German\n",
    "polex=pd.read_csv(\"/home/klenner/Lehre/ml20/polexNeutNomen\",header=None,\n",
    "                  index_col=[0,1],usecols=[0,1],names=['lemma', 'pol'])\n",
    "\n",
    "\n",
    "ids=[]     # gather all embedding indices\n",
    "index={}   # map index to polarity\n",
    "oov=0      # out of vocabolary counter\n",
    "\n",
    "for (l,p),_ in polex.iterrows():  # (lemma,polarity) pairs\n",
    "    try:\n",
    "        id=widx[l]  \n",
    "        if p=='POS' or p=='PRO':\n",
    "            index[id]=1     # id is the word2vec index of lemma l\n",
    "            ids.append(id)  # all ids for data split below\n",
    "        elif p=='NEG' or p=='CON':\n",
    "            index[id]=0\n",
    "            ids.append(id)\n",
    "        else:\n",
    "            index[id]=2\n",
    "            ids.append(id)\n",
    "    except:\n",
    "        oov+=1\n",
    "        pass\n",
    "\n",
    "shuffle(ids,random_state=0)                    # random modifies ids directly\n",
    "X=[emb[id] for id in ids]                 \n",
    "y=[index[id] for id in ids]      \n",
    "len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "perceptron = Perceptron(tol=1e-3)\n",
    "perceptron.fit(X, y)\n",
    "\n",
    "svm = svm.SVC(kernel='linear', C=1.0, random_state=0)\n",
    "svm.fit(X,y)\n",
    "\n",
    "mlp = MLPClassifier(solver='adam', alpha=1e-5, activation='relu',\n",
    "                    hidden_layer_sizes=(300,10), random_state=1,validation_fraction=0.2,early_stopping=True)            \n",
    "mlp.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Statement\n",
    "\n",
    "cross validation produces stable accuracy results\n",
    "\n",
    "* we compare 5-fold to 10-fold to 50-fold cross validation\n",
    "* cross validation\n",
    "    * split the data set into n folds\n",
    "    * take each fold once as test and \n",
    "    * n-1 times as part of training\n",
    "    * take the average of results\n",
    "    \n",
    "the average scores vary only slightly, they are stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(perceptron, X, y, cv=5)\n",
    "\n",
    "print(\"all scores:\",scores,\"\\naverage score of 5 fold cv:\",scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_scores10 = cross_val_score(perceptron, X, y, cv=10)\n",
    "print(\"10 fold cv:\",perceptron_scores10.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(perceptron, X, y, cv=50)\n",
    "scores.mean()\n",
    "print(\"20 fold cv:\",scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Statement\n",
    "\n",
    "since cv results are stable, we can more reliably apply the t-test\n",
    "\n",
    "* we compare mlp with perceptron and svm\n",
    "* we do it with the the results of the n folds\n",
    "    * this is valid only if the folds are determined equally in each run \n",
    "    * otherwise the test folds could differ and we don't have proper pairs \n",
    "    * reason: we are applying the paired t-test\n",
    "    * sklearn seems to guarantee this (see next field)\n",
    "    * sklearn produces stratified folds, i.e. the class distribution is preserved in each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we run is twice, the result don't differ\n",
    "# i.e. given a corpus, every splitting into folds gives the same result, crucial for t-test comparision\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "Xcv = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "ycv = np.array([0, 0, 1, 1])\n",
    "\n",
    "skf = StratifiedKFold(n_splits=2)\n",
    "skf.get_n_splits(Xcv, ycv)\n",
    "\n",
    "print(skf)\n",
    "\n",
    "for train_index, test_index in skf.split(Xcv, ycv):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = Xcv[train_index], Xcv[test_index]\n",
    "    y_train, y_test = ycv[train_index], ycv[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we compare mlp to perceptron\n",
    "\n",
    "perceptron_scores10 = cross_val_score(perceptron, X, y, cv=10)\n",
    "res_perceptron=perceptron_scores10.mean()\n",
    "\n",
    "mlp_scores10 = cross_val_score(mlp, X, y, cv=10)\n",
    "res_mlp=mlp_scores10.mean()\n",
    "\n",
    "print(\"mlp:\",res_mlp,\"perceptron:\",res_perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mlp scores 10 fold cv\\n\", mlp_scores10,\"\\n\\nperceptron scores 10 fold cv\\n\",perceptron_scores10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "        $\\hat\\sigma_{\\bar{x}}$= $\\frac{{\\hat{\\sigma}}\\approx s}{\\sqrt{n}}$\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paired t-test, null hypothesis is: both classifier have equal performance\n",
    "from scipy.stats import t\n",
    "\n",
    "# degress of freedom\n",
    "df=10  # number of folds\n",
    "length=df\n",
    "\n",
    "print(\"99% of values are smaller than\",t.ppf(0.99,df-1)) \n",
    "\n",
    "# substract the accuracy scores and fix the mean\n",
    "diff=mlp_scores10-perceptron_scores10\n",
    "mean=diff.mean()\n",
    "\n",
    "# find the t-value\n",
    "t_val=mean/(np.std(diff,ddof=1)/np.sqrt(length))\n",
    "\n",
    "print(\"\\nt_val is higher than this, namely:\",t_val,\"- we thus reject that both have equal performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's compare mlp with svm\n",
    "\n",
    "mlp_scores10 = cross_val_score(mlp, X, y, cv=10)\n",
    "res_mlp=mlp_scores10.mean()\n",
    "svm_scores10 = cross_val_score(svm, X, y, cv=10)\n",
    "res_svm=svm_scores10.mean()\n",
    "\n",
    "print(\"mlp:\",res_mlp,\"svm:\",res_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff=mlp_scores10-svm_scores10\n",
    "mean=diff.mean()\n",
    "\n",
    "t_val=mean/(np.std(diff)/np.sqrt(length))  \n",
    "\n",
    "print(\"95% of values are smaller than\",t.ppf(0.95,df-1)) \n",
    "\n",
    "print(\"\\nt_val is smaller than this, namely:\",t_val,\"- we thus cannot reject the null hypothesis\")\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
